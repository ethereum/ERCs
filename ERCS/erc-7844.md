---
eip: 7844
title: Consolidated Dynamic Storage (CDS)
description: Storage abstraction layer enabling schema-safe upgrades via extendable structs and expandable storage spaces with standardized access patterns.
author: Cameron Warnick (@wisecameron)
discussions-to: https://ethereum-magicians.org/t/erc-7844-consolidated-dynamic-storage-cds/22217
status: Draft
type: Standards Track
category: ERC
created: 2024-12-13
---

## Abstract  

**Consolidated Dynamic Storage (CDS)** is a custom storage primitive that enables in-place, schema-safe upgrades of contract state without slot collisions, broken dependencies, or ABI mismatches. It introduces extendible structs—dynamically expandable storage layouts—organized into hash-segregated storage spaces with deterministic slot derivation and standardized access routines. These capabilities allow contracts to evolve their storage layout without impacting existing deployments or integrations.

CDS is not just an upgrade pattern, but a comprehensive storage abstraction layer that decouples logic from state while unifying access across contracts. Its structure eliminates manual slot planning, custom getters, and fragile upgrade paths by centralizing storage operations through a single interface. Developers can define, extend, and access structured data in-place, allowing seamless evolution of protocol storage over time, without the risk or overhead of redeploying logic contracts.

Critically, CDS enforces consistent storage semantics, access patterns, and permission-gated security measures, enabling seamless cross-organizational interoperability, easier audits, and more robust ecosystem integrations. Its role-based access control system supports granular permission management, making it suitable for autonomous governance and DAO-controlled upgrades. Together, these features make CDS a scalable foundation for long-lived systems—such as DAOs, DeFi protocols, and modular contract suites—where safe extensibility, composability, and low-maintenance upgrades are essential.

![Architecture](../assets/erc-7844/Architecture.svg)

## Motivation  

### CDS Overview
The motivation behind CDS is multi-faceted.  Centrally, the proposal focuses on establishing easy access to consistent, error-free upgradeability, and abstracting away risky manual storage management in favor of deterministic function operations.  However, accomplishing full storage abstraction and in-place upgradeability requires significant implementation effort: for example, implementing extendible structs in a palatable form necessitates support for Solidity types, complex access patterns, and a custom hashing system to avoid collisions and provide support for struct extension.  Owing to this natural scope creep and its runoff effects, CDS has blossomed into a holistic storage management framework rather than a traditional design pattern, and within this basis, numerous additional benefits -- including standardized access patterns, guaranteed single-hop storage proximity, streamlined dependencies, the obsoletion of putters, getters, and adapters -- have also manifested.

### Storage Abstraction: Why?
As previously stated, storage abstraction is the *root benefit* of CDS: even with contemporary best practices, developers still risk mis-alignments, broken dependencies, and other high-impact issues when fielding contract upgrades.  While it's natural to instinctively dismiss these issues as being relevant only to novice devleopers and teams, the reality is that they are perhaps even more likely in the scope of complex protocols with novel utility.  Furthermore, the future growth of the ecosystem will almost certainly lead to a surge in new developers, who may not fully understand the intricacies and potential dangers of EVM storage. At the very least, resolutely eliminating the possibility for these critical issues streamlines the scope and mental load associated with upgrades, and upgrading storage without re-deployment is both more convenient and cost-effective.  These benefits are compounded when considering **storage consolidation:** CDS is the *single source of truth for its data* and imposes standardized access patterns.  Upgrades are guaranteed to be smoothly propagated without breaking dependencies or requiring adapter logic.  This is particularly beneficial in the case of cross-organizational interoperability, where close coordination can prove especially challenging.

### Understanding the Meaning of "Holistic Storage Management"
The contemporary Ethereum development landscape has reached an interesting place: many core problems and pain points have been identified and resolved by patterns defined in individual ERC proposals, where most proposals address a specific problem and leave it to developers to integrate and maintain them in production.  However, implementing, cohesively coordinating, and effectively leveraging several distinct patterns can be burdensome, especially as protocols evolve.  

Some proposals address niche, application-specific issues: most protocols don't have any use for them, but they have a significant impact when they are relevant.  However, others follow a different paradigm, providing solid approaches to issues with high prevalence, where the only real downside is only the effort to implement and coordinate the solution. More clearly, almost *any* protocol would likely benefit from leveraging the pattern, because it addresses a fundamental inefficiency.  Following this thread, the fact that every large project isn't using straightfoward, yet immensely useful patterns such as the [Dependency Registry](./erc-6224.md) highlights the problem addressed by taking broader strokes: awareness of each pattern isn't universal, and it *does* take effort to understand and leverage them in production.  

The above context is necessary to both position and substantiate the unique approach CDS takes. It's not a design pattern: a CDS layer is a complete, standalone contract.  Developers don't need to perform any under-the-hood tinkering: rather, they deploy a CDS *instance* and use it to store, modify, and retrieve data.  Considering that CDS cohesively echoes best practices introduced in a number of other proposals, this approach can be understood as a solution to the growing disconnect between fragmented solutions and the run-of-the-mill Ethereum developer.  More simply, the purpose of holistic storage management, and arguably CDS in general, is to make good code hygeine and advanced capabilities such as upgradeability accessible, safe, and *practical*. It's simply a fact of nature that most people do not dig deep: many great solutions will remain niche regardless of ecosystem growth and adoption.  Only a minority will ever consider that cross-contract dependencies can rapidly grow in complexity due to siloed contract logic and storage, that storage access signatures lack any standardization, or that upgrading struct fields can break dependant function selectors. 

### Why Standardize CDS With an ERC Proposal?
CDS is worth standardizing because it can be understood as a new *foundational storage primitive.*  Fundamentally, a CDS contract could be abstractly viewed as a unique data structure, akin to a dynamic array containing mappings of the custom extendible struct data structure.  Furthermore, CDS is designed to optimize cross-contract interoperability: consistency between separate models is critical.  For example, a developer may integrate their contract with another organization's CDS layer, granting one of their contracts modification privileges to a specific field in a storage space.  If the partner implementation deviates from the spec, this could cause unexpected behavior. 

### Harmonizing Upgradeability, Control, and Decentralization
While storage abstraction has become the prevailing narrative behind CDS, it was initially conceived to answer the following question: how can we balance upgradeability and decentralization?  Of course, the answer is to gate upgradeability behind autonomously-executing user governance.  The CDS spec demands that RBAC is included.  Consequently, developers can fully delegate control to governance layers, allowing the team to field re-deployments and reshape data structures, but without the clear threat to system integrity posed by such control in an unguarded state.  The primary intuition behind the CDS approach is that the absence of needed capabilities is an uncomfortable position: and its severity is exacerbated by the immutability of deployed bytecode.  The landscape evolves rapidly, and it isn't hard to out-compete a static protocol.  Smart contracts are named as such because they are autonomously-executing, static "agreements."  However, real-world contracts can be amended, just not by a single involved party: static protocols are actually less adaptable than real-world contracts.  Through integrated governance, CDS provably maintains the "contract" invariant, and its design makes it easy to achieve this state.  We should also note that many cases, the complexity of achieving provably-safe upgradeability is too high to justify its inclusion, despite the fact that it would be a valuable addition.  CDS bridges both gaps, giving an olive branch to teams who prioritize provable decentralization, even in lieu of upgradeability.  

### Problems that CDS Resolutely Solves
1. **Fractured Cross-Contract Links** 
	Adding new fields to existing *struct* types can cause a variety of complications in the domain of cross-contract interoperability.  

	*Function Selector Changes:* If a function takes *struct* as a parameter, changes to the struct layout will change its function selector.  External to the now-deprecated selector will route to the fallback function (or simply fail) until those contracts are re-deployed.  Coordinating re-deployments for all dependent contracts can be challenging, especially in cross-organizational settings.

	*Broken Return Data Encoding:* If a function returns a struct, even if it’s not passed as an argument, changes to its structure invalidate the expected return data encoding.  Any function expecting the old layout will receive extra bytes, causing the ABI decoder to revert the transaction.

	*Calldata Decoding Failures:* Struct modifications also affect how externally supplied calldata is parsed. Functions that rely on ABI encoding to decode struct parameters will break if the expected data structure changes, leading to silent failures or unexpected execution paths. This is particularly problematic for modular contract ecosystems, where various contracts interact via cross-contract calls and must interpret the same struct definitions consistently.

2.	**Compulsory Redeployments** 
	Updating storage structures is not universally supported. Even when Proxy-Delegate architectures are used, storage modifications almost always require contract redeployments. Expanding storage structures without altering contract bytecode is possible in theory but impractical in most real-world scenarios—any functional change, such as new getters or modified logic, necessitates redeployment.

	This imposes direct costs in both gas and operational complexity. Additionally, modifying existing contract logic can raise security concerns among users who rely on the protocol’s integrity. For protocols governed by decentralized mechanisms, complexities including coordinating upgrades across disparate governance layers can introduce further friction, making even minor storage modifications a potentially time-consuming and costly process.

3.	**Lack of Standardization in Access Routines** 
	No two protocols handle storage in exactly the same way: each introduces unique struct layouts, slot assignments, and upgrade management strategies. As a result, integrating with external contracts requires adapting to different storage access models.

	Unlike standardized token interfaces, storage lacks a universal querying system. Contracts frequently rely on inefficient or delicate solutions including:

	* Hard-coded storage slot references, which introduce fragility when upgrades modify slot positions.
	* Mirrored state variables, which duplicate data inefficiently, increasing gas costs.
	* Custom getter/setter functions, which increase deployment costs while requiring integrators to implement protocol-specific logic to retrieve state.  

	More fundamentally, access patterns may be subject to change—an integration built for one contract version may break when the underlying storage structure is updated, requiring maintenance across multiple versions.

4. **Manual Storage Management** 
	Storage updates are inherently high-risk due to Solidity’s strict alignment rules. Even when following best practices, upgradeable storage models require careful management to avoid introducing permanent corruption.

	While Proxy-Delegate and Diamond Storage architectures provide upgrade paths, they require explicit tracking of storage layouts. Misaligned struct modifications or incorrect slot allocations can cause irrecoverable state corruption.

	Additionally, the need for manual slot assignments and upgrade procedures introduces a security risk—any miscalculation in struct offsets, slot positions, or initialization routines can lead to unintended storage overwrites. While careful audits and best practices reduce this risk, they do not eliminate it entirely.

	It is true that best practices can largely mitigate the risks associated with storage updates. However, the potential for breaking changes remains a technical burden in itself—ensuring upgrade safety requires significant time, effort, and expertise, particularly in complex ecosystems.

5.  **Siloed Storage**
	On-chain storage is often tied to individual contracts rather than a unified data management layer.  This leads to complex linked structures and multi-hop access patterns, where operations require data aggregated from multiple external dependencies.  This added complexity and computational overhead is purely a burden, and can be avoided by leveraging dedicated storage layers with a cross-contract scope, as is often seen in the Diamond Pattern.  

6.  **Long-Term Technical Debt and Maintainability** 
	Factors including adapters, siloed storage, multi-hops, and dependent interfaces introduce new risks and considerations, increasing the complexity of upgrades as ecosystem scope expands.  The absence of these considerations streamlines the upgrade process considerably, allowing developers to prioritize optimizing and securing critical business logic.  Additionally, increased lines of code and complexity typically garner higher audit costs, while increasing the odds that vulnerable code makes it to production.


## Specification 

![Adaptability](../assets/erc-7844/Adaptability.svg)

The keywords “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “RECOMMENDED”, “MAY” and “OPTIONAL” in this document are to be interpreted as described in RFC 2119.

### Extendable Structs and Storage Spaces
- Extendable structs **MUST** leverage dynamic mappings with deterministic field hashes (`keccak256`).  
- The base struct and its additions **MUST** remain immutable, while fields **MUST** be dynamically appendable.
- Storage spaces **MUST** equate to a simple extension of the hashing structure, segmenting both struct-defining and active data.

Each struct member **MUST** be defined using the following compact metadata structures:

| **Field**  | **Bits** | **Description**                          |
| ---------- | -------- | ---------------------------------------- |
| `bitCount` | 128      | Starting bit offset for the member.      |
| `size`     | 64       | Size of the member in bits.              |
| `type`     | 64       | Type ID (e.g., `uint256`, `bool`, etc.). |

### Type IDs

| **Type**                   | **ID** | **Size**     |
| -------------------------- | ------ | ------------ |
| `uint`                     | 1      | 8...256 bits |
| `int`                      | 2      | 8...256 bits |
| `bool`                     | 3      | 8 bits       |
| `address`                  | 4      | 160 bits     |
| `bytes32 (optional)`       | 5      | 256 bits     |
| `string (≡ bytes)`         | 6      | Dynamic      |

*For arrays, developers **MAY** define unpacking logic to treat `string` or `bytes` fields as indexed collections of dynamic elements.*

### Hash Structure
There are three main segments that **MUST** include separate hashing structures to resolve any potential for collisions, meta-segmented by their particular storage spaces.  These are: storage space state data and member-specific data, storage space live data, and storage space dynamic data. 

There are two special values that **MUST** be included: a `safeIndex` and a `stringIndex`.  Both are expounded upon in detail below.

Developers **SHOULD** include a unique hash offset for each space, as this simplifies the development and audit process considerably, and thereby greatly reduces the risk of improper implementation.

A `storageSpace` `offset` value **MUST** be included.  This is explicitly marked in the below section for clarity.  

*MEMBERS_LIM* refers to the total max quantity of struct members permitted within a single extendible struct.  
*ENTRIES_LIM* refers to the max amount of entries in a storage space, and serves as the upper limit for *push* and *pushMany*.  

`A`, `B`, `C` hash offset structure not only allows CDS to functionally guarantee safety from collisions, but also aid in debugging by 
allowing maintainers to instantly track the high-level objective of complex storage operations.

**Metadata Root:**  
`offset := shl(176, mul(storageSpaces, MEMBERS_LIM))`  
`mstore(0x0, offset)`  
`ROOT_SLOT := keccak256(0x0, 0xA)`  

**Storage Space State Data:**  
`sload(ROOT_SLOT)` → {`members(64), entries(64), stringIndex(64), safeIndex (64)`}  

**Storage Space Member-Specific Data:**  
`sload(add(ROOT_SLOT, add(1, memberIndex)))` → {`bitCount(128), size(64), type(64)`}   
*String*: `sload(add(ROOT_SLOT, add(1, memberIndex)))` → {`stringIndex(128), type(128)`}  

![Storage Metadata](../assets/erc-7844/FirstInitCreateStorageMetadata.svg)

**Live Data:**  
`mstore(0x0, shl(168, add(entryIndex, mul(ENTRIES_LIM, storageSpace))))`  
`INDEXED_DATA_ROOT_IN_STORAGE_SPACE = keccak256(0x0, 0xB)` → {`packed slot`}    
Use `bitCount` and size (in member-specific data) to derive the location of a  
desired struct member in an indexed storage space.  

![Live Data Storage](../assets/erc-7844/LiveDataStorage.svg)

**Storage Space Dynamic Data:**  
`[strindex (32)][entryIndex (32)][storageSpace (32)]`  
```solidity
        mstore(
            0x0,
            or(
                or(shl(224, entryStrindex), shl(192, entryIndex)),
                shl(160, and(storageSpace, 0xFFFFFFFF))
            )
        )
        ENTRY_SLOT := keccak256(0x0, 0xC)
```

**Storage Space Dynamic Data Length:**  

```solidity
	root := sload(ENTRY_SLOT)
	len := shr(224, sload(ENTRY_SLOT))
```  
The length of strings (which functionally double as arrays) is stored in the root slot, with data stored afterwards.  We use this layout because it simplifies storage operations and conversion to the native string type.
![String Layout](../assets/erc-7844/StringLayout.svg)

**entryIndex, memberIndex**:  
*entryIndex* **MUST** refer to a mapped index.  Note that CDS operates with sequential indexing: 0, 1, 2, 3, etc.  
*memberIndex* **MUST** refer to a particular struct member.  

**High-level Solidity equivalent (memberIndex, entryIndex):**

```solidity
	struct Dog
	{
		string furColor;    //memberIndex 0
		string eyeColor;    //memberIndex 1
		uint128 toothCount; //memberIndex 2
		uint128 legsCount;  //memberIndex 3
	}

	mapping(uint256 => Dog) dogs;
	dogs[6 /*entryIndex*/].furColor /*memberIndex 0*/ = 'brown';
```

For example, a *put* invocation **MUST** look like: *put(data, memberIndex, entryIndex, storageSpace)*. 

*stringIndex, safeIndex*:
String index **MUST** be utilized to separate strings in the contract storage space.  Given that strings MUST have a dynamic size, they do not use `bitCount`.  Hence, we MUST fill their `bitCount` with `stringIndex` instead in member data.

However, this raises a problem when implementing extendable structs: *if the last member is a string, we might reference back to its `memberIndex`, tricking the system into believing the `stringIndex` is a `bitCount`.* Hence, we **MUST** leverage `safeIndex`, which records the most recent valid `memberIndex` we can use to derive `bitCount`.  Critically, if `safeIndex` is zero, we are still safe from complications, because a zero `strindex` doubles as a valid `bitCount` in that instance: there are zero preceding bits.

Example Implementation:
```solidity
function insert_new_member(
	uint256 valType,
	uint256 valSize,
	uint256 storageSpace
) external
{
	verify type and size
	retrieve memberData
	
	if(type not string)
	{
		get safeIndex
		assign bitCount := prev bitCount + sizeof previous
		verify size
		get storage page
		verify we will not overflow
		if overflow, push to next page (update bitCount to head of next page)
		pack memberData
		store memberData
		update safeIndex, members in state data for storage space
	}
	if(type is string)
	{
		get stringIndex
		pack with type
		store packed metadata in memberData
		increment stringIndex, members
		store updated state data for storage space
	}

}
```

**Permission Management:**  
Developers **MUST** include a permission management scheme for their CDS model.  They **MAY** utilize the following basic structure:  

| **Scope**                  | **Level** | **Significance**               |
| -------------------------- | ------    | ------------------------------ |
| `User`                     | 1         | Permission Management          |
| `User`                     | 2         | Modify Data                    |
| `User`                     | 3         | Modify Storage Structures      |
| `Contract`                 | 4         | Permission Management          |
| `Contract`                 | 5         | Modify Data                    |
| `Contract`                 | 6         | Modify Storage Structures      |
| `Universal`                | 7         | Full Access                    |


### Interfaces
```solidity
    function init_create(
    uint256[] memory types, 
    uint256[] memory sizes
) external;

    function insert_new_member(
    uint256 valType, 
    uint256 size, 
    uint256 storageSpace
) external;

function push(
	uint256 storageSpace
) external;

function pushMany(
    uint256 amount, 
    uint256 storageSpace
) external;

function put(
    uint256 data, 
    uint256 memberIndex, 
    uint256 entryIndex, 
    uint256 storageSpace
) external;

function put_string(
	string memory data, 
	uint256 memberIndex, 
	uint256 entryIndex, 
	uint256 storageSpace
) external;

function get(
    uint256 memberIndex, 
    uint256 entryIndex, 
    uint256 storageSpace
) external view returns(uint256);

function get_string(
    uint256 memberIndex, 
    uint256 entryIndex, 
    uint256 storageSpace
) external view returns(string memory returnValue);

function set_permissions(
	uint256 level,
	address recipient
) external;

function get_storage_space_state_data(
    uint256 storageSpace
) external view returns(
	uint256 members, 
	uint256 entries,
	uint256 stringIndex,
	uint256 safeIndex
);

function total_members(
    uint256 storageSpace
) external view returns(uint256);

/**
* For string, bitCount->stringIndex, size param is extraneous.
*/
function get_member_data(
	uint256 memberIndex,
	uint256 storageSpace
) external view returns(
	uint256 bitCount, 
	uint256 valSize,
	uint256 valType, 
);

function get_permission_level(
	address target
) external view returns(uint256);

```

### Optional Interface Members
```solidity 
function put_batch(
   uint256[] memory values, 
   uint256[] memory members, 
   uint256 entryIndex, 
   uint256 storageSpace
) external;

function get_batch(
    uint256[] memory members, 
    uint256 entryIndex, 
    uint256 storageSpace
) external view returns(uint256[] memory result);

function get_index_from_address(
	address indexAddress,
	uint256 memberIndex,
	uint256 storageSpace
) external view returns(uint256 entryIndex);

function map_address_to_index(
	address indexAddress,
	uint256 storageSpace,
	uint256 targetIndex
) internal;

function put_with_address(
	address indexAddress,
	uint256 storageSpace,
) external;

function _get_root_slot(
	uint256 storageSpace
) internal view returns(uint256);

```
### Initialization

![Basic Interaction Flow](../assets/erc-7844/BasicInteractionFlow.svg)
![CDS Init, Extension](../assets/erc-7844/CDSInitExpansion.svg)  
The `init_create` function **MUST** handle storage space creation in both an initialization and live extension setting.  The function **MUST** take an array of types and sizes as input, which **MUST** conform to the above specifications, and be equivalent in length when input.  Developers **MUST** include length equivalency validation for `types` and `sizes`.

```solidity
function init_create(
	uint256[] memory types,
	uint256[] memory sizes,
)
{
	/*
		Recommended:
		storageSpaces += 1;
		ROOT_SLOT = _get_root_slot(storageSpaces - 1);
	*/
	for(i in range sizes)
	{
		if(types[i] in [1..5])
		{
			validate size given type
			calculate bitCount for new entry
			pack {bitCount, types, sizes}
			store the packed value in the member data of the storage space
			bitCount := bitCount + size
			increase safeIndex
		}
		if(types[i] is 6)
		{
			create packed value {stringIndex, 6}
			store the packed value in the member data of the storage space
			increment stringIndex
		}
	}
	pack storage space data: {members, entries, stringIndex, safeIndex}
	store storage space data
	storageSpaces += 1
}
```

## Rationale  
Proxy-Delegate and Diamond Storage architectures have long been the industry standard for upgradeable smart contracts, providing critical mechanisms for separating logic from state. These approaches have enabled modular contract upgrades, mitigated contract size limitations, and allowed systems to evolve over time. However, as protocols scale, new challenges emerge—managing storage across multiple contracts, coordinating upgrades, and ensuring compatibility without introducing technical debt or unnecessary redeployments.

CDS builds upon these established patterns by introducing a dedicated, structured storage layer that eliminates many of the complexities inherent in Proxy-Delegate and Diamond architectures. In traditional models, storage modifications require careful slot management, pre-planning reserved fields, or governance-heavy migrations to maintain compatibility. These constraints impose operational overhead and increase the risk of misalignment between logic and storage. CDS removes these limitations by enabling in-place schema evolution, ensuring that storage structures expand deterministically without slot corruption, ABI misalignment, or redundant redeployments.

One of the key improvements CDS introduces is standardized storage access across contracts. Whereas Proxy-Delegate and Diamond models require each contract to define its own storage mappings, CDS provides a shared, structured interface that multiple contracts can access consistently. This reduces cross-contract storage fragmentation, simplifies state synchronization, and eliminates the inefficiencies of multi-hop storage dependencies and redundant mappings.

Furthermore, CDS is designed with long-term system maintainability in mind. As protocols expand, managing state across interdependent contracts becomes increasingly complex. CDS mitigates these risks by externalizing structured storage into a permissioned, extendable layer, ensuring that high-throughput contract interactions remain efficient even as ecosystems grow in scale and complexity.

Proxy-Delegate and Diamond architectures remain valuable for many upgradeable contract designs, offering modularity and logic flexibility. However, for protocols requiring sustainable long-term upgradeability, efficient cross-contract storage, and deterministic schema evolution, CDS provides a more scalable and future-proof alternative. By eliminating migration risks and reducing upgrade complexity, CDS ensures storage integrity, seamless interoperability, and efficient state management—empowering developers to focus on core logic rather than maintenance overhead.

## Backwards Compatibility  

This ERC introduces a new design pattern and does not interfere with existing Solidity implementations. CDS *does* *not* implicitly interfere with common libraries such as those provided by OpenZeppelin, but is not supported explicitly. Library-imposed global data within CDS-linked contracts can be a burden if it is not refactored to link to your CDS layer.

## **Test Cases**

### **1. Core Functionality**

- **Initialization**
    - Input: `types = [1, 3, 6], sizes = [32, 8, 128]`.
    	- Expected: Storage space initialized with 3 members.
- **Insert New Members**
    - Input: `insert_new_member(1, 128, storageSpace = 0)`.
    	- Expected: New `uint128` member added with correct `bitCount`.
- **Data Storage and Retrieval**
    - Input: `put(42, memberIndex = 0, entryIndex = 0, storageSpace = 0)` → `get(0, 0, 0)`.
    	- Expected: `42`.

### **2. Edge Cases**

- **String Handling**
    - Input: Insert five strings consecutively.
    	- Expected: No collisions; strings retrieved accurately.
    
    - Input: insert two dynamic strings, then one uint256
		- Expected: uint256 is properly configured with:
				`bitCount == 0` 
			because: 
				`safeIndex == 0` maps to the dynamic string with index `0`.  This zero value fills both decoded `{type, size}` slots in the standard type construction logic.  Hence, we begin with a valid `bitCount` of `0`.
		    
- **Entry Creation**
    - Input: Add `10,000` entries to a storage space with `pushMany`.
    	- Expected: System can store to any of these entry indices.
    
- **Invalid Input**
    - Input: `put(42, memberIndex = 1, storageSpace = 0)`.
    	- Expected: Reverts with error.
    
	* Input: `put("42", memberIndex = 1, entryIndex = 0, storageSpace = 0)`.
    	- Expected: Reverts with error.
    
	- Input: `put_string(42, memberIndex = 1, entryIndex = 0, storageSpace = 0)`.
    	- Expected: Reverts with error.

### Gas Benchmarks

* This section assumes that storage operations interact with pre-populated slots.  
* These values are derived from the optimized HoneyBadger model and serve as good targets for efficiency.  
* Displayed values reference *execution* cost.  

	`init_create([1], [256]):` 93,860 gas  
	`init_create([1,1,1], [8,256,256]):` 140,024 gas  
	`insert_new_member:` 40,653 gas  
	`push:` 10,316 gas  
	`put:` 15,543 gas  
	`put_batch([20, 20], [0, 1], 0, 0):` 22,895 gas  
	`get`: 9374 gas  

## Reference Implementation

Refer to [CDS Minimal Example](../assets/erc-7844/CDSMinimal.sol)

### Notable Downsides and Mitigating Solutions

While CDS represents a definite step of progress towards efficient, unfettered scalability, 
it does bring about challenges beyond the relative difficulty of implementing a working model.

### Address Indexing
**Impact:** The standard CDS model is locked to uint indexing, which is restrictive for numerous usecases.

**Solution:** The core model does not natively support address indexing (i.e.; *mapping(address => uint256)*), as this inclusion would excessively bloat implementation overhead if included in the baseline model. 

However, CDS can be implemented with a *mapping(uint256 => mapping(address => uint256))*, 
-- representing storageSpace -> address index -> entryIndex -- and an additional mapping 
from *uint256* to *bool* that flags storage spaces as address-indexed.  Contract calls to 
address-indexed storage spaces should first resolve the entryIndex by invoking *get_index_from_address* before interacting with the system as normal.  

To ensure that address-indexed storage spaces are properly populated with addresses, 
it is recommended to leverage the *put_with_address* function when populating 
the storage space. 

While this approach is cumbersome, it effectively introduces address indexing without 
requiring significant additional effort.  

To support other non-uint-indexing capabilities, it is recommended to leverage the same pattern as is used for address indexing. 

### Rigidity of CDS Itself
**Impact:** Standard CDS models are unable to accommodate logical extensions, which can restrict autonomy in a production setting.

**Solution:** As the above example demonstrates, there are cases where extending the baseline CDS implementation is desirable.  Accomplishing this simply requires leveraging the proxy-delegate model for your CDS implementation.

### Non-Descriptive Syntax
**Impact:** One of the most common complaints about CDS is that it effectively masks operations behind a non-descriptive syntax (i.e.; *put(1200, 0, 0, 2)).*  

**Solution:** The best way to mitigate this issue is to pair the non-descriptive syntax with descriptive comments and liberal use of Enums.  

By defining an Enum to name the members of an extendible struct, and another to encode storage spaces, we can significantly enhance the readability of our CDS syntax with little effort.

**Without Enums:** *put(123, 0,0, 1);*  
**With Enums:** *put(123, Globals.TotalBottles, 0, StorageSpaces.Globals);*  


### Storage Space Permanence
**Impact:** Mistakes in defining storage space members are permanent.

**Solution:** While this seems like a significant problem at face-value, it is actually relatively benign because *CDS is leveraged by linked contracts, not users.*  Hence, mistaken struct members can simply be denoted as "inert" within the master docstring and removed from linked contracts.  

## Security Considerations

While a CDS implementation imposes considerable overhead in terms of up-front effort, comprehensively addressing security risks is relatively straightforward.

### Hash Structure and Masking 
Validate your hash structure and masks stringently.  It is recommended to manually verify each operation at least once, using an external document to track verified operations.  Fuzzing key operations is also highly-advised.  While the widespread utilization of complex assembly operations seems daunting, this process is more tedious than it is complicated.  The key is to be diligent - it will either work, or it won't.  If it doesn't work, unit tests and fuzzing will demonstrate that.  While the system is complex under the hood, errors are easy to identify in practice, and they generally arise from improper assembly-level operations.

### Access Control 
It is highly recommended to fully lock the system with a permission management scheme.  The most basic example would be to use a `mapping(address => boolean) hasPerms` with a related modifier.  Users who lack authorization should never have direct access to the system, including view functions.  In the case of views, we prevent access because there is no need for non-users to have insight into your storage space scheme.  It is recommended to use an abstraction layer via an additional contract to ensure that this information is abstracted away, and that users can't pry into data they shouldn't have access to.

### Types and Sizes 
Types and sizes also represent an important vulnerability point.  It is crucial to verify that types are within bounds, and to thoroughly verify that entries conform to expected size(s).  It is best practice to revert when encountering an incorrect size (i.e.; boolean, size 16) rather than correcting it in-place.

## Copyright

Copyright and related rights waived via [CC0](../LICENSE.md).
